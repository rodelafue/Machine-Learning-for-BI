{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ayudantía 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta ayudantía retomaremos lo dejado en el problema anterior y veremos los siguientes tópicos:\n",
    " - método `__str__`\n",
    " - Pipeline\n",
    " - Wranglers\n",
    " - Regresión logística\n",
    " - ejercicio de practica\n",
    " - usos de librería pandas\n",
    " - usos de libreria matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero escribamos una clase para hacer el ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Punto(object):\n",
    "    \"\"\" Representación de un punto en el plano, los atributos son x e y\n",
    "        que representan los valores de las coordenadas cartesianas.\"\"\"\n",
    "    def __init__(self, x=0, y=0):\n",
    "        \"\"\" Constructor de Punto, x e y deben ser numéricos,\n",
    "            de no ser así, se levanta una excepción TypeError \"\"\"\n",
    "        if self.es_numero(x) and self.es_numero(y):\n",
    "            self.x=x\n",
    "            self.y=y\n",
    "        else:\n",
    "            raise TypeError(\"x e y deben ser valores numéricos\")\n",
    "            \n",
    "    def es_numero(self,valor):\n",
    "        \"\"\" Indica si un valor es numérico o no. \"\"\"\n",
    "        return isinstance(valor, (int, float, complex) )\n",
    "    \n",
    "    def distancia(self, otro):\n",
    "        \"\"\" Devuelve la distancia entre ambos puntos. \"\"\"\n",
    "        dx = self.x - otro.x\n",
    "        dy = self.y - otro.y\n",
    "        return (dx*dx + dy*dy)**0.5\n",
    "    \n",
    "    def restar(self, otro):\n",
    "        \"\"\" Devuelve un nuevo punto, con la resta entre dos puntos. \"\"\"\n",
    "        return Punto(self.x - otro.x, self.y - otro.y)\n",
    "    \n",
    "    def norma(self):\n",
    "        \"\"\" Devuelve la norma del vector que va desde el origen\n",
    "            hasta el punto. \"\"\"\n",
    "        return (self.x*self.x + self.y*self.y)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además recordemos uno de los métodos especiales, `__init_`, el constructor de la clase. Este método se llama cada vez que se crea una nueva instancia de la clase.\n",
    "\n",
    "Para definir atributos, basta con definir una variable dentro de la instancia utilizando `self.`, es una buena idea definir todos los atributos de nuestras instancias en el constructor, de modo que se creen con algún valor válido. En nuestro ejemplo `self.x` y `self.y` y se usarán como  `punto.x` y `punto.y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Punto(5,7)\n",
    "q = Punto(2,3)\n",
    "r = p.restar(q)\n",
    "print(r.x, r.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.norma())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q.distancia(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Un método para mostrar objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar objetos, Python indica que hay que agregarle a la clase un método especial, llamado *`__str__`* que debe devolver una cadena de caracteres con lo que queremos mostrar. Ese método se invoca cada vez que se llama a la función `str`.\n",
    "\n",
    "El método *`__str__`* tiene un solo parámetro, `self`.\n",
    "\n",
    "En nuestro caso decidimos mostrar el punto como un par ordenado, por lo que escribimos el siguiente método dentro de la clase Punto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Punto(object):\n",
    "    \"\"\" Representación de un punto en el plano, los atributos son x e y\n",
    "        que representan los valores de las coordenadas cartesianas.\"\"\"\n",
    "    def __init__(self, x=0, y=0):\n",
    "        \"\"\" Constructor de Punto, x e y deben ser numéricos,\n",
    "            de no ser así, se levanta una excepción TypeError \"\"\"\n",
    "        if self.es_numero(x) and self.es_numero(y):\n",
    "            self.x=x\n",
    "            self.y=y\n",
    "        else:\n",
    "            raise TypeError(\"x e y deben ser valores numéricos\")\n",
    "            \n",
    "    def es_numero(self,valor):\n",
    "        \"\"\" Indica si un valor es numérico o no. \"\"\"\n",
    "        return isinstance(valor, (int, float, complex) )\n",
    "    \n",
    "    def distancia(self, otro):\n",
    "        \"\"\" Devuelve la distancia entre ambos puntos. \"\"\"\n",
    "        dx = self.x - otro.x\n",
    "        dy = self.y - otro.y\n",
    "        return (dx*dx + dy*dy)**0.5\n",
    "    \n",
    "    def restar(self, otro):\n",
    "        \"\"\" Devuelve un nuevo punto, con la resta entre dos puntos. \"\"\"\n",
    "        return Punto(self.x - otro.x, self.y - otro.y)\n",
    "    \n",
    "    def norma(self):\n",
    "        \"\"\" Devuelve la norma del vector que va desde el origen\n",
    "            hasta el punto. \"\"\"\n",
    "        return (self.x*self.x + self.y*self.y)**0.5\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\" Muestra el punto como un par ordenado. \"\"\"\n",
    "        return \"(\" + str(self.x) + \", \" + str(self.y) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Punto(-6,18)\n",
    "str(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Métodos para operar matemáticamente\n",
    "Ya hemos visto un método que permitía restar dos puntos. Si bien esta implementación es perfectamente válida, no es posible usar esa función para realizar una resta con el operador -."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Punto(3,4)\n",
    "q = Punto(2,5)\n",
    "print(p - q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos que este operador (o el equivalente para la suma) funcione, será necesario implementar algunos métodos especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Punto(object):\n",
    "    \"\"\" Representación de un punto en el plano, los atributos son x e y\n",
    "        que representan los valores de las coordenadas cartesianas.\"\"\"\n",
    "    def __init__(self, x=0, y=0):\n",
    "        \"\"\" Constructor de Punto, x e y deben ser numéricos,\n",
    "            de no ser así, se levanta una excepción TypeError \"\"\"\n",
    "        if self.es_numero(x) and self.es_numero(y):\n",
    "            self.x=x\n",
    "            self.y=y\n",
    "        else:\n",
    "            raise TypeError(\"x e y deben ser valores numéricos\")\n",
    "            \n",
    "    def es_numero(self,valor):\n",
    "        \"\"\" Indica si un valor es numérico o no. \"\"\"\n",
    "        return isinstance(valor, (int, float, complex) )\n",
    "    \n",
    "    def distancia(self, otro):\n",
    "        \"\"\" Devuelve la distancia entre ambos puntos. \"\"\"\n",
    "        dx = self.x - otro.x\n",
    "        dy = self.y - otro.y\n",
    "        return (dx*dx + dy*dy)**0.5\n",
    "    \n",
    "    def restar(self, otro):\n",
    "        \"\"\" Devuelve un nuevo punto, con la resta entre dos puntos. \"\"\"\n",
    "        return Punto(self.x - otro.x, self.y - otro.y)\n",
    "    \n",
    "    def norma(self):\n",
    "        \"\"\" Devuelve la norma del vector que va desde el origen\n",
    "            hasta el punto. \"\"\"\n",
    "        return (self.x*self.x + self.y*self.y)**0.5\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\" Muestra el punto como un par ordenado. \"\"\"\n",
    "        return \"(\" + str(self.x) + \", \" + str(self.y) + \")\"\n",
    "\n",
    "    def __add__(self, otro):\n",
    "        \"\"\" Devuelve la suma de ambos puntos. \"\"\"\n",
    "        return Punto(self.x + otro.x, self.y + otro.y)\n",
    "\n",
    "    def __sub__(self, otro):\n",
    "        \"\"\" Devuelve la resta de ambos puntos. \"\"\"\n",
    "        return Punto(self.x - otro.x, self.y - otro.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método *`__add__`* es el que se utiliza para el operador +, el primer parámetro es el primer operando de la suma, y el segundo parámetro el segundo operando. Debe devolver una nueva instancia, nunca modificar la clase actual. De la misma forma, el método *`__sub__`* es el utilizado por el operador -.\n",
    "\n",
    "Ahora es posible operar con los puntos directamente mediante los operadores, en lugar de llamar a métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Punto(3,4)\n",
    "q = Punto(2,5)\n",
    "print( p - q )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( p + q )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmq = p - q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pmq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seguir practicando sobre la creación de clases, puedes revisar [está página](https://uniwebsidad.com/libros/algoritmos-python/capitulo-14/creando-clases-mas-complejas) (en español)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHAT?\n",
    "\n",
    "Pipelines allow you to create a single object that includes all steps from data preprocessing and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHY?\n",
    "\n",
    "* Increase reproducibility\n",
    "* Make it easier to use cross validation and other types of model selection.\n",
    "* Avoid common mistakes such as leaking data from training sets into test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest possible Pipeline\n",
    "Just a classifier and one preprocessing step (data standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression,make_classification\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100,n_features=10,n_informative=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a list of tuples as parameter\n",
    "pipeline = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# use the pipeline object as you would\n",
    "# a regular classifier\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_preds = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "Dealing with messy data.\n",
    "\n",
    "* Google is your best friend.\n",
    "\n",
    "\n",
    "## Python\n",
    "\n",
    "Libraries needed:\n",
    "* numpy\n",
    "* pandas\n",
    "* matplotlib\n",
    "* scikit-learn\n",
    "\n",
    "\n",
    "### Recomendations:\n",
    "\n",
    "6.0001 Introduction to Computer Science and Programming in Python:\n",
    "- https://www.youtube.com/playlist?list=PLUl4u3cNGP63WbdFxL8giv4yhgdMGaZNA\n",
    "- https://courses.edx.org/courses/course-v1:MITx+6.00.1x+2T2017_2/course/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MovieLens data\n",
    "\n",
    "http://grouplens.org/datasets/movielens/\n",
    "\n",
    "Example inspired by Verena Kaynig-Fittkau & [Greg Reda](http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/)\n",
    "\n",
    "We start by loading the [users](http://files.grouplens.org/datasets/movielens/ml-100k/u.user), [ratings](http://files.grouplens.org/datasets/movielens/ml-100k/u.data), and [movies](http://files.grouplens.org/datasets/movielens/ml-100k/u.item) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the users data\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "\n",
    "users = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/u.user', \n",
    "        sep='|', names = u_cols)\n",
    "users.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ratings data\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/u.data', sep='\\t', names=r_cols)\n",
    "\n",
    "ratings.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movies data\n",
    "\n",
    "# the movies file contains columns indicating the movie's genres\n",
    "# let's only load the first five columns of the file with usecols\n",
    "m_cols = ['movie_id', 'title', 'release_date','video_release_date', 'imdb_url']\n",
    "\n",
    "movies = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/u.item',\n",
    "                     sep='|', names=m_cols,  usecols=range(5), encoding='iso-8859-1')\n",
    "\n",
    "movies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get info about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies.dtypes)\n",
    "print(\"------------\")\n",
    "print(movies.describe())\n",
    "# *** Why only those two columns? ***\n",
    "print(\"------------\")\n",
    "print(ratings.describe())\n",
    "print(\"------------\")\n",
    "print(users.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create one merged DataFrame\n",
    "\n",
    "movie_ratings = pd.merge(movies, ratings)\n",
    "lens = pd.merge(movie_ratings, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data\n",
    "\n",
    "* DataFrame => group of Series with shared index\n",
    "* single DataFrame column => Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['occupation'].unique() #unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting more than one column\n",
    "columns_you_want = ['occupation', 'sex'] \n",
    "users[columns_you_want].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show users over 18\n",
    "adultUsers =   users[users.age > 18]\n",
    "adultUsers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show teenagers\n",
    "teenUsers = users[(users.age >= 13) & (users.age<20)]\n",
    "teenUsers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more complex filters\n",
    "* Show male teenagers\n",
    "* Show the mean age of female programmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show male teenagers\n",
    "print(users[(users.sex == 'M') & (users.age>=10) &(users.age<20)].head())\n",
    "\n",
    "print('------------')\n",
    "\n",
    "# Show the mean age of female programmers\n",
    "print(users[(users.sex == 'F')]['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "What are the most rated movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.title.value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_lens = lens.groupby(['movie_id','title']).size().sort_values(ascending=False)\n",
    "sorted_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting\n",
    "\n",
    "A pivot table is a table that summarizes data in another table, and is made by applying an operation such as sorting, averaging, or summing to data in the first table, typically including grouping of the data.\n",
    "\n",
    "Let's calculate the mean rating for men and women for each film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = lens.pivot_table(index=['movie_id', 'title'],\n",
    "                           columns=['sex'],\n",
    "                           values='rating',\n",
    "                           fill_value=0)\n",
    "\n",
    "#pivot_table() function has the mean as a default operation.\n",
    "\n",
    "\n",
    "pivoted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* list all occupations and how many mens and females works on they."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all occupations and how many mens and females works on they.\n",
    "users.groupby([ 'occupation','sex']).size().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística sobre datos normales multivariados:\n",
    "Queremos hacer una regresión logística de manera de discriminar entre datos que provienen de una distribución normal multivariada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#Crearemos los datos:\n",
    "\n",
    "mu = np.array([[0.5, 0.5],\n",
    "              [0.75, 0.8],\n",
    "              [1, 0.75]])\n",
    "S = np.array([[[0.02, 0],\n",
    "             [0, 0.01]],\n",
    "             [[0.02, 0.01],\n",
    "             [0.01, 0.02]],\n",
    "             [[0.01, 0],\n",
    "             [0, 0.02]]])\n",
    "\n",
    "K = mu.shape[0]\n",
    "mvn = []\n",
    "x = []\n",
    "N = 100\n",
    "for i in range(mu.shape[0]):\n",
    "    mvn.append(multivariate_normal(mean=mu[i], cov=S[i]))\n",
    "    x.append(mvn[i].rvs(N))\n",
    "\n",
    "bins = 100\n",
    "grid_x1 = np.linspace(0, 1.5, bins)\n",
    "grid_x2 = np.linspace(0, 1.5, bins)\n",
    "x1_v, x2_v = np.meshgrid(grid_x1, grid_x2)\n",
    "\n",
    "def plotMVN (mvn):\n",
    "    K = len(mvn)\n",
    "    for i in range(K):\n",
    "        P = mvn[i].pdf(np.array([x1_v.flatten(), x2_v.flatten()]).transpose())\n",
    "        plt.contour(grid_x1, grid_x2, P.reshape ([bins, bins]),\n",
    "               levels=np.array([0.2, 0.5, 0.9])*P.max(),\n",
    "               colors=colors[i])\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize =(4,4))\n",
    "colors = [\"blue\", \"red\", \"green\"]\n",
    "plotMVN (mvn)\n",
    "for i in range(K):\n",
    "    plt.plot (x[i][:, 0], x[i][:, 1], \"+\", color = colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación Binaria\n",
    "Partiremos con una clasificación binaria utilizando sólo dos clases: $K=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvn2 = mvn[0:2]\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize =(4,4))\n",
    "plotMVN (mvn2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Regresión Logística intenta estimar la probabilidad $p(C_1|\\boldsymbol{x})$ utilizando una función llamada función logistica o función Sigmoide\n",
    "\n",
    "$p(C_1|\\boldsymbol{x}) = y(\\boldsymbol{x}) = \\sigma (w_0 + w_1x_1 + \\cdots + w_mx_m)= \\sigma(\\boldsymbol{w}^\\top\\boldsymbol{\\phi})$,\n",
    "\n",
    "Donde: \n",
    "\n",
    "\n",
    "$\\sigma(a) = \\frac{1}{1+\\exp(-a)} = \\frac{\\exp(a)}{1 + \\exp(a)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(-10, 10, 0.1)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(a, 1./(1.+np.exp(-a)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de máxima verosimilitud para Regresión Logística\n",
    "\n",
    "\n",
    "Dado $\\boldsymbol{x}$, la probabilidad de ser de clase $C_1$ es $\\sigma(\\boldsymbol{w}^\\top\\boldsymbol{x} + w_0)$. Por simplicidad, utilizamos la variable aleatoria $y \\in \\{0, 1\\}$, donde $y = 1$ cuando el objeto es de clase $C_1$, y $y=0$ cuando el objeto es de clase $C_2$. En ese sentido, podemos definir:\n",
    "\n",
    "$p(y|\\boldsymbol{x}) = \\sigma^y(1-\\sigma)^{1-y}$,\n",
    "\n",
    "Donde hemos omitido $\\boldsymbol{w}^\\top\\boldsymbol{\\phi}$ para simplificar la notación. Asumiendo que las observaciones son independientes, podemos obtener la función de verosimilitud:\n",
    "\n",
    "$p(\\boldsymbol{y}|\\boldsymbol{w}) = \\prod_{n = 1}^N \\sigma_n^{y_n}(1-\\sigma_n)^{1-y_n}$\n",
    "\n",
    "Donde $\\{(\\boldsymbol{x}_n, y_n)\\}_{n = 1}^N$ representa los atributos y sus respectivas etiquetas, y además $\\sigma_n = \\sigma(\\boldsymbol{w}^\\top\\boldsymbol{\\phi_n})$. Podemos definitir una función de error tomando el negativo del logaritmo de la verosimilitud, el cual nos da el error de _entropia-cruzada_, de la forma:\n",
    "\n",
    "\n",
    "$E(\\boldsymbol{w}) = -\\ln p(\\boldsymbol{y}|\\boldsymbol{w}) = -\\sum_{n=1}^N \\{y_n\\ln \\sigma_n + (1-y_n)\\ln (1-\\sigma_n)\\}$.\n",
    "\n",
    "Esto es lo que queremos minimizar respecto a los parámetros $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 50\n",
    "N2 = 50\n",
    "x2 = np.concatenate ((mvn[0].rvs(N1), mvn[1].rvs(N2)))\n",
    "y2 = np.zeros(N1 + N2)\n",
    "y2[:N1] = 1\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter (x2[:, 0], x2[:,1], c = (1-y2), alpha = 0.8, cmap=plt.cm.jet)\n",
    "plotMVN (mvn2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(x2,y2)\n",
    "\n",
    "y_pred = logreg.predict(np.c_[x1_v.ravel(), x2_v.ravel()])\n",
    "y_pred = y_pred.reshape((bins, bins))\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.contourf(x1_v, x2_v, y_pred, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter (x2[:, 0], x2[:,1], c = (1-y2), alpha = 0.5, cmap=plt.cm.jet)\n",
    "plotMVN (mvn2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística Regularizada\n",
    "\n",
    "Recordando, queremos optimizar el negativo del logaritmo de la verosimilitud:\n",
    "\n",
    "$E(\\boldsymbol{w}) = -\\ln p(\\boldsymbol{y}|\\boldsymbol{w}) = -\\sum_{n=1}^N \\{y_n\\ln \\sigma_n + (1-y_n)\\ln (1-\\sigma_n)\\}$.\n",
    "\n",
    "**Occam's razor:** Entre las hipótesis que explican el mismo experimento, la más simple debería ser seleccionada.\n",
    "\n",
    "Podemos añadir una penalización a esta función de verosimilitud para forzar la obtención de modelos más simples:\n",
    "\n",
    "$E(\\boldsymbol{w}) = -\\sum_{n=1}^N \\{y_n\\ln \\sigma_n + (1-y_n)\\ln (1-\\sigma_n)\\} + \\lambda^2\\left(||\\boldsymbol{w}||_p\\right)^p$,\n",
    "\n",
    "Donde $||\\boldsymbol{w}||_p = \\left(\\sum_{j=1}^M |w_i|^p\\right)^{1/p}$. Por ejemplo, para $p = 1$ : \n",
    "\n",
    "$E(\\boldsymbol{w}) = -\\sum_{n=1}^N \\{y_n\\ln \\sigma_n + (1-y_n)\\ln (1-\\sigma_n)\\} + \\lambda^2\\sum_{j=1}^M |w_j|$,\n",
    "\n",
    "La cual es llamada regularización L1 para Regresión logística.\n",
    "\n",
    "Para $p = 2$ : \n",
    "\n",
    "$E(\\boldsymbol{w}) = -\\sum_{n=1}^N \\{y_n\\ln \\sigma_n + (1-y_n)\\ln (1-\\sigma_n)\\} + \\lambda^2\\sum_{j=1}^M w_j^2$,\n",
    "\n",
    "Llamada regularización L2 para regresión logística. \n",
    "\n",
    "En la práctica, el intercepto no es parte del factor de penalización, sino forzaríamos a que este fuese cero.\n",
    "\n",
    "En scikit-learn el término regularizador corresponde a la constate $C$, equivalente a $1/\\lambda^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 1e5)\n",
    "logreg.fit(x2,y2)\n",
    "\n",
    "y_pred = logreg.predict(np.c_[x1_v.ravel(), x2_v.ravel()])\n",
    "y_pred = y_pred.reshape((bins, bins))\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.contourf(x1_v, x2_v, y_pred, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter (x2[:, 0], x2[:,1], c = (1-y2), alpha = 0.5, cmap=plt.cm.jet)\n",
    "plotMVN (mvn2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística multiclase:\n",
    "\n",
    "Ahora utilizaremos los datos provenientes de las 3 funciones normales multivariadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = np.concatenate ((x[0], x[1], x[2]))\n",
    "y3 = np.zeros(3*N)\n",
    "y3[N:2*N] = 1\n",
    "y3[2*N:] = 2\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize =(4,4))\n",
    "colors = [\"blue\", \"green\", \"red\"]\n",
    "plotMVN (mvn)\n",
    "plt.scatter (x3[:, 0], x3[:,1], c = (y3), alpha = 0.8, cmap=plt.cm.jet)\n",
    "plotMVN (mvn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión logística multiclase es llamada por defecto \"Una contra todos\".\n",
    "Si tenemos 3 clases, entonces ajustamos 3 regresiones logísticas, donde la probabilidad de cada categoría es predicha sobre el resto de las categorías combinadas. En nuestro caso, necesitamos entrenar 3 modelos:\n",
    "\n",
    "1. $C_1$ contra $C_2$ y $C_3$ combinados\n",
    "2. $C_2$ contra $C_1$ y $C_3$ combinados\n",
    "3. $C_3$ contra $C_2$ y $C_1$ combinados\n",
    "\n",
    "Scikit-learn tiene esto implementado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 1)\n",
    "logreg.fit(x3,y3)\n",
    "\n",
    "y_pred = logreg.predict(np.c_[x1_v.ravel(), x2_v.ravel()])\n",
    "y_pred = y_pred.reshape((bins, bins))\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.contourf(x1_v, x2_v, y_pred, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter (x3[:, 0], x3[:,1], c = (y3), alpha = 0.5, cmap=plt.cm.jet)\n",
    "plotMVN (mvn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora nos vamos con un ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://stats.idre.ucla.edu/stat/data/binary.csv')\n",
    "df.head()\n",
    "# Base de datos de postulantes de postgrado a UCLA. La columna admit define si el estudiante es aceptado o no.\n",
    "# 0: No es aceptad, 1: es aceptado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debemos cambiar el nombre de la columna \"rank\" por \"prestige\", pues rank es un método de pandas.\n",
    "df.columns = [\"admit\", \"gre\", \"gpa\", \"prestige\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los histogramas son muchas veces una de las herramientas más importantes en un análisis exploratorio.\n",
    "# Son capaces de darnos de una manera muy simple una interpretación de como están distribuidos nuestros datos.\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['admit'], df['prestige'], rownames=['admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_ranks = pd.get_dummies(df['prestige'], prefix='prestige', drop_first=True)\n",
    "dummy_ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['admit', 'gre', 'gpa']\n",
    "data = df[cols_to_keep].join(dummy_ranks.loc[:, 'prestige_2':])\n",
    "data['intercept'] = 1.0\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[data.columns[1:]]\n",
    "y = data['admit']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea\n",
    "Utilizando el método `train_test_split` entregue el vector *y* y la matrix *X* con un tamaño del subset test en 30% (`test_size=0.3`) y elija una semilla de 12 para poder comparar los resultados con los demás (`random_state=12`). Recuerda que el único parámetro que hemos vistode regresión logística es el inverso de la regularización, pruebe con los siguientes valores para ver que tan importante es este parámetro `params = [1,10,50,100,500,1000,10000]`.\n",
    "\n",
    "Para ver cual es el mejor utilice el método `logit.score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un método efectivo y visual para ver que tan bien nuestro modelo está efectuando la decisión es por medio de la matrix de confusión, aca tienes un código para crear una matrix de confunción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, save_as='conf_mtrx_interp', \n",
    "                          use_sm = -1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Este método plotea la matrix de confusión dado las siguientes features\n",
    "    \n",
    "    cm= metodo confusion matrix con los datos y los predichos confusion_matrix(y_test,y_pred), \n",
    "    classes=un vector que defina todas las clases,\n",
    "    title= titulo de la matrix de confusión, por default 'Confusion matrix',\n",
    "    cmap=paleta de colores, por default plt.cm.Blues, \n",
    "    save_as=si quieres guardalo, por default'conf_mtrx_interp', \n",
    "    use_sm = -1)\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.clf()\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilizando la predicción dada por el modelo plotea la matrix de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un ejemplo de la matrix de confusión\n",
    "Para que veas como usarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "c=10e3\n",
    "lr = LogisticRegression(C = c)\n",
    "sc = cross_val_score(lr, x, y, cv=10)*100\n",
    "print(\"Regresión logística = \", np.mean(sc), \" +- \", np.std(sc))\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=77)\n",
    "logit = LogisticRegression(C=c)\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_test)\n",
    "confu = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names=['Iris-Setosa','Iris-Versicolour','Iris-Virginica']\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confu, classes=class_names,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, puedes utilizar otros métodos de prueba de predicción como `f1 score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fl_score\n",
    "print(fl_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Más revisión de Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Handling Missing Data\n",
    "The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous.\n",
    "In particular, many interesting datasets will have some amount of data missing.\n",
    "To make matters even more complicated, different data sources may indicate missing data in different ways.\n",
    "\n",
    "In this section, we will discuss some general considerations for missing data, discuss how Pandas chooses to represent it, and demonstrate some built-in Pandas tools for handling missing data in Python.\n",
    "Here and throughout the book, we'll refer to missing data in general as *null*, *NaN*, or *NA* values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Trade-Offs in Missing Data Conventions\n",
    "\n",
    "There are a number of schemes that have been developed to indicate the presence of missing data in a table or DataFrame.\n",
    "Generally, they revolve around one of two strategies: using a *mask* that globally indicates missing values, or choosing a *sentinel value* that indicates a missing entry.\n",
    "\n",
    "In the masking approach, the mask might be an entirely separate Boolean array, or it may involve appropriation of one bit in the data representation to locally indicate the null status of a value.\n",
    "\n",
    "In the sentinel approach, the sentinel value could be some data-specific convention, such as indicating a missing integer value with -9999 or some rare bit pattern, or it could be a more global convention, such as indicating a missing floating-point value with NaN (Not a Number), a special value which is part of the IEEE floating-point specification.\n",
    "\n",
    "None of these approaches is without trade-offs: use of a separate mask array requires allocation of an additional Boolean array, which adds overhead in both storage and computation. A sentinel value reduces the range of valid values that can be represented, and may require extra (often non-optimized) logic in CPU and GPU arithmetic. Common special values like NaN are not available for all data types.\n",
    "\n",
    "As in most cases where no universally optimal choice exists, different languages and systems use different conventions.\n",
    "For example, the R language uses reserved bit patterns within each data type as sentinel values indicating missing data, while the SciDB system uses an extra byte attached to every cell which indicates a NA state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Missing Data in Pandas\n",
    "\n",
    "The way in which Pandas handles missing values is constrained by its reliance on the NumPy package, which does not have a built-in notion of NA values for non-floating-point data types.\n",
    "\n",
    "Pandas could have followed R's lead in specifying bit patterns for each individual data type to indicate nullness, but this approach turns out to be rather unwieldy.\n",
    "While R contains four basic data types, NumPy supports *far* more than this: for example, while R has a single integer type, NumPy supports *fourteen* basic integer types once you account for available precisions, signedness, and endianness of the encoding.\n",
    "Reserving a specific bit pattern in all available NumPy types would lead to an unwieldy amount of overhead in special-casing various operations for various types, likely even requiring a new fork of the NumPy package. Further, for the smaller data types (such as 8-bit integers), sacrificing a bit to use as a mask will significantly reduce the range of values it can represent.\n",
    "\n",
    "NumPy does have support for masked arrays – that is, arrays that have a separate Boolean mask array attached for marking data as \"good\" or \"bad.\"\n",
    "Pandas could have derived from this, but the overhead in both storage, computation, and code maintenance makes that an unattractive choice.\n",
    "\n",
    "With these constraints in mind, Pandas chose to use sentinels for missing data, and further chose to use two already-existing Python null values: the special floating-point ``NaN`` value, and the Python ``None`` object.\n",
    "This choice has some side effects, as we will see, but in practice ends up being a good compromise in most cases of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ``None``: Pythonic missing data\n",
    "\n",
    "The first sentinel value used by Pandas is ``None``, a Python singleton object that is often used for missing data in Python code.\n",
    "Because it is a Python object, ``None`` cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type ``'object'`` (i.e., arrays of Python objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.array([1, None, 3, 4])\n",
    "vals1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This ``dtype=object`` means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects.\n",
    "While this kind of object array is useful for some purposes, any operations on the data will be done at the Python level, with much more overhead than the typically fast operations seen for arrays with native types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['object', 'int']:\n",
    "    print(\"dtype =\", dtype)\n",
    "    %timeit np.arange(1E6, dtype=dtype).sum()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of Python objects in an array also means that if you perform aggregations like ``sum()`` or ``min()`` across an array with a ``None`` value, you will generally get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reflects the fact that addition between an integer and ``None`` is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``NaN``: Missing numerical data\n",
    "\n",
    "The other missing data representation, ``NaN`` (acronym for *Not a Number*), is different; it is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = np.array([1, np.nan, 3, 4]) \n",
    "vals2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NumPy chose a native floating-point type for this array: this means that unlike the object array from before, this array supports fast operations pushed into compiled code.\n",
    "You should be aware that ``NaN`` is a bit like a data virus–it infects any other object it touches.\n",
    "Regardless of the operation, the result of arithmetic with ``NaN`` will be another ``NaN``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 *  np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this means that aggregates over the values are well defined (i.e., they don't result in an error) but not always useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.sum(), vals2.min(), vals2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy does provide some special aggregations that will ignore these missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that ``NaN`` is specifically a floating-point value; there is no equivalent NaN value for integers, strings, or other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN and None in Pandas\n",
    "\n",
    "``NaN`` and ``None`` both have their place, and Pandas is built to handle the two of them nearly interchangeably, converting between them where appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, np.nan, 2, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For types that don't have an available sentinel value, Pandas automatically type-casts when NA values are present.\n",
    "For example, if we set a value in an integer array to ``np.nan``, it will automatically be upcast to a floating-point type to accommodate the NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(range(2), dtype=int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0] = None\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in addition to casting the integer array to floating point, Pandas automatically converts the ``None`` to a ``NaN`` value.\n",
    "(Be aware that there is a proposal to add a native integer NA to Pandas in the future; as of this writing, it has not been included).\n",
    "\n",
    "While this type of magic may feel a bit hackish compared to the more unified approach to NA values in domain-specific languages like R, the Pandas sentinel/casting approach works quite well in practice and in my experience only rarely causes issues.\n",
    "\n",
    "The following table lists the upcasting conventions in Pandas when NA values are introduced:\n",
    "\n",
    "|Typeclass     | Conversion When Storing NAs | NA Sentinel Value      |\n",
    "|--------------|-----------------------------|------------------------|\n",
    "| ``floating`` | No change                   | ``np.nan``             |\n",
    "| ``object``   | No change                   | ``None`` or ``np.nan`` |\n",
    "| ``integer``  | Cast to ``float64``         | ``np.nan``             |\n",
    "| ``boolean``  | Cast to ``object``          | ``None`` or ``np.nan`` |\n",
    "\n",
    "Keep in mind that in Pandas, string data is always stored with an ``object`` dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on Null Values\n",
    "\n",
    "As we have seen, Pandas treats ``None`` and ``NaN`` as essentially interchangeable for indicating missing or null values.\n",
    "To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures.\n",
    "They are:\n",
    "\n",
    "- ``isnull()``: Generate a boolean mask indicating missing values\n",
    "- ``notnull()``: Opposite of ``isnull()``\n",
    "- ``dropna()``: Return a filtered version of the data\n",
    "- ``fillna()``: Return a copy of the data with missing values filled or imputed\n",
    "\n",
    "We will conclude this section with a brief exploration and demonstration of these routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting null values\n",
    "Pandas data structures have two useful methods for detecting null data: ``isnull()`` and ``notnull()``.\n",
    "Either one will return a Boolean mask over the data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 'hello', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in Data Indexing and Selection, Boolean masks can be used directly as a ``Series`` or ``DataFrame`` index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``isnull()`` and ``notnull()`` methods produce similar Boolean results for ``DataFrame``s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping null values\n",
    "\n",
    "In addition to the masking used before, there are the convenience methods, ``dropna()``\n",
    "(which removes NA values) and ``fillna()`` (which fills in NA values). For a ``Series``,\n",
    "the result is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a ``DataFrame``, there are more options.\n",
    "Consider the following ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,      np.nan, 2],\n",
    "                   [2,      3,      5],\n",
    "                   [np.nan, 4,      6]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot drop single values from a ``DataFrame``; we can only drop full rows or full columns.\n",
    "Depending on the application, you might want one or the other, so ``dropna()`` gives a number of options for a ``DataFrame``.\n",
    "\n",
    "By default, ``dropna()`` will drop all rows in which *any* null value is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can drop NA values along a different axis; ``axis=1`` drops all columns containing a null value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this drops some good data as well; you might rather be interested in dropping rows or columns with *all* NA values, or a majority of NA values.\n",
    "This can be specified through the ``how`` or ``thresh`` parameters, which allow fine control of the number of nulls to allow through.\n",
    "\n",
    "The default is ``how='any'``, such that any row or column (depending on the ``axis`` keyword) containing a null value will be dropped.\n",
    "You can also specify ``how='all'``, which will only drop rows/columns that are *all* null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns', how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finer-grained control, the ``thresh`` parameter lets you specify a minimum number of non-null values for the row/column to be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='rows', thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first and last row have been dropped, because they contain only two non-null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling null values\n",
    "\n",
    "Sometimes rather than dropping NA values, you'd rather replace them with a valid value.\n",
    "This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values.\n",
    "You could do this in-place using the ``isnull()`` method as a mask, but because it is such a common operation Pandas provides the ``fillna()`` method, which returns a copy of the array with the null values replaced.\n",
    "\n",
    "Consider the following ``Series``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill NA entries with a single value, such as zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a forward-fill to propagate the previous value forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward-fill\n",
    "data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can specify a back-fill to propagate the next values backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-fill\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ``DataFrame``s, the options are similar, but we can also specify an ``axis`` along which the fills take place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Respuesta Tarea\n",
    "for c in params:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=77)\n",
    "    logit = LogisticRegression(C=c)\n",
    "    logit.fit(X_train, y_train)\n",
    "    print('Accuracy: {:.2f}'.format(logit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "confu = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names=['Admitted','Non Admitted']\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confu, classes=class_names,\n",
    "                      title='Normalized confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
